\chapter{Research Results}
In this chapter, we will discuss the results of the research. We will discuss
the sampling algorithm for the Attraction Indian buffet process (AIBP) and
derive the p.m.f. of the distribution. Using a simulation study, we will also
compare the properties of the IBP, AIBP, and ddIBP.

\section{Notation for the AIBP}
Before discussing the sampling algorithm and the p.m.f. for the AIBP,  we will 
discuss the notation that will be used. In this section, the term ``customer" will
be used to refer to a row (or observation) in a dataset; the term ``dish" will
likewise be used to refer to a column in a dataset. This language is also used in
the IBP and ddIBP.

\noindent
Let:
\begin{itemize}
  \item $\bm \sigma = (\sigma_1,..\sigma_n)$ of ${1,...n}$ gives the sequence
        in which the n customers are allocated, where the $t^{th}$ customer
        allocated is $\sigma_t$. We will call $\bm \sigma$ the
        \textit{permutation} of the data.  The order of customers in the
        dataset is not necessarily the order that the observations will be
        allocated in implementation. Note that $\bm \sigma = {1,...,n}$ when 
        the customers are allocated in the order they appear in the dataset.

  \item $\lambda(i,j)$ = similarity between customers $i$ and $j$. The function
        $\lambda$ maps the distance between customers to a measure of how
        ``close" together the customers are. If $d_{ij}$ represents the distance
        between customers $i$ and $j$, then $\lambda(i,j)=f(d_{ij})$ should be a
        non-decreasing function bounded below by 0. Examples of 
        $\lambda(i,j)=f(d_{ij})$ include: (i) reciprocal similarity $f(d) = 1/d$,
        and (ii) exponential similarity $f(d) = exp(-d)$.

  \item $x_i$ = the number of \textit{new} dishes that customer $i$ draws.

  \item $y_{-i} = \suml{j}{1}{i-1} x_j$, the number of existing dishes
        before customer $i$ draws new dishes, for $i=\{2,...,N\}$. We will
        define $y_{-i} = 0$.

  \item $g_i(x_i) = \ds\frac{(\alpha/i)^{x_i}\exp(-\alpha/i)}{x_i!}$, 
        which is the probability mass function for a Poisson distribution 
        with parameter $\alpha/i$ and argument $x_i$.

  \item $K$ = the number of (non-zero) columns in $\bm Z$.

  \item $m_{-i,k}$ = the number of customers that took dish $k$ before customer
        customer $i$ samples dishes.

  \item $m_{-i} = \suml{k}{1}{K}m_{-i,k}$, which is the total number of dishes 
        taken before customer $i$ samples dishes.

  \item $h_{i,k} = \ds\frac{\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)}
        {\suml{k}{1}{y_i}\left(\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)
         \right)}$, the weight given to dish $k$ for customer $i$. \\

  \item $p_{i,k} = h_{i,k} \ds\frac{m_{-i}}{i}$, the probability of customer
        $i$ drawing dish $k$. Note that when $\lambda$ is a constant, $p_{i,k}$
        reduces to $m_{-i,k}$ as in the IBP.
  %\item $h(i) = \prodl{k}{1}{y_{-i}} 
  %      (p_{i,k})^{z_{i,k}}(1-p_{i,k})^{1-z_{i,k}}$, for $i>1$,
  %      and $1$ for $i=1$.

  \item $H_N = \suml{i}{1}{N} \ds\frac{1}{i}$, the harmonic number.
\end{itemize}


\section{Sampling Algorithm for the AIBP}
We will now introduce the sampling algorithm for the AIBP for a given
permutation $\bm\sigma$. The sampling algorithm is similar to that of the IBP,
with minor changes to the sampling of previously sampled dishes. We will use 
$\bm Z$ to denote a realization from the AIBP with parameter $\alpha$, where 
$\alpha$ is a mass parameter that governs the number of dishes that will be drawn.
The larger $\alpha$ is, the larger the number of total dishes drawn ($K$) will be.\\

\noindent
To obtain a realization $\bm Z$ from an AIBP($\alpha$), the first customer
draws an $x_1 \sim$ Poisson($\alpha$) number of dishes. This is indicated by
setting $z_{1,1:x_1}$ to $1$. For each subsequent customer $i$, if there exists
any previously sampled dishes (i.e. if $y_{-i}>0$), customer $i$ samples dish $k$
with probability $\left(h_{i,k}\right)$, for $k \in \{1,...,y_{-i}\}$. The customer
then samples an additional $x_i \sim$ Poisson($\alpha/i$) number of new dishes. 

%\begin{itemize}
%  \item The first customer draws an $x_1=$ Poisson($\alpha$) number of dishes. This
%        is indicated by setting $z_{1,1:x_1}$ to $1$.
%  \item For the next $i$ customers, for $i=1:N$, 
%     \begin{itemize}
%       \item if there exists previously sampled dishes, then customer $i$
%             samples dish k with probability $\left(h_{i,k}\right) 
%             \left(\ds\frac{m_{-i}}{i}\right)$, for $ k = 1:y_{-i}$.
%       \item customer $i$ draws another Poisson($\alpha/i$) number of new dishes.
%     \end{itemize}
%\end{itemize}


\section{Probability Mass Function}
Based on the sampling algorithm above, we can write down the p.m.f. for the
proposed distribution as a product of independent probabilities for 
each customer and for each dish. Here we will define 
$\prodl{k}{1}{y_{-i}} (p_{i,k})^{z_{i,k}}(1-p_{i,k})^{1-z_{i,k}}=1$ if $y_{-i}=0$.

\begin{align*}
  \text{P}(\bm Z|\bm\sigma)
  =&\prodl{i}{1}{N} \left\{g_i(x_i) \prodl{k}{1}{y_{-i}} 
    (p_{i,k})^{z_{i,k}}(1-p_{i,k})^{1-z_{i,k}}\right\}\\
  =&\prodl{i}{1}{N} \left\{ \ds\frac{(\alpha/i)^{x_i}\exp(-\alpha/i)}{x_i!} 
    \prodl{k}{1}{y_{-i}} 
    \ds\frac{(h_{i,k}m{-i})^{z_{i,k}}(i-h_{i,k}m_{-i,k})^{1-z_{i,k}}}{i} 
    \right\} \\
  =&\ds\frac{\alpha^{\suml{i}{1}{N}x_i} \exp(-\alpha H_N)} {\prodl{i}{1}{N}x_i!} 
    \left( \prodl{i}{1}{N}i^{-x_i} \right) \times
    \\
    &\left( \prodl{i}{1}{N}\prodl{k}{1}{y_{-i}} i^{-1}
    \ds\left(\hik m_{-i}\right)^{z_{i,k}}
       \left(i-\hik m_{-i}\right)^{1-z_{i,k}} \right)\\
   &\\
  =&\ds\frac{\alpha^K \exp(-\alpha H_N)} {\prodl{i}{1}{N}x_i!} \times
    \\
    &\left( \prodl{i}{2}{N} i^{-x_i-y_{-i}}\prodl{k}{1}{y_{-i}} 
    \ds\frac{
    \left(\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)m_{-i}\right)^{z_{i,k}}
    \left(i-\sums{\sigma}{z_{-i,k}=1}{}
    \lambda(\sigma,\sigma_i)m_{-i}\right)^{1-z_{i,k}}
    }
    {\suml{k}{1}{y_{-i}}\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)}\right)
\end{align*}

\section{Properties of the AIBP}
In the literature review, it was shown how each of the distributions that made
use of pairwise distance information could be reduced to the original
distributions (CRP and IBP). The AIBP can likewise be reduced to the IBP when
the similarity function is constant.\\
\noindent
One of the properties we wished to preserve from the IBP was that the expected
number of dishes taken by each customer would be $\alpha$. The algorithm
prescribed above allocates dishes to customer based on their proximity to other
customers.  That is if two customers are close together, they are more likely
to share the same dishes. The number of new dishes sampled by each customer in
the AIBP is the same as that in the IBP, by construction. So it remains to show
that the expected number of previously-sampled dishes $r_i$ sampled by by
customer $i$ in the AIBP is the same as that in the IBP. In the IBP, the
expected row sum for the first row is $\alpha$, which is the same as in the
AIBP. So the expected row sum is the same for the first row. For subsequent
rows, the expected value of $r_i$ in the IBP is E$\left[\suml{k}{1}{y_{-i}}
\frac{m_{-i,k}}{i}\right]$ = E$\left[\frac{m_{-i}}{i}\right]$ =
$\frac{\alpha(i-1)}{i}$. For the AIBP, the expected value of $r_i$ is:
\[
  E\left[\suml{k}{1}{y_{-i}}\hik \frac{m_{-i}}{i}\right] = 
  E\left[\frac{m_{-i}}{i}\right] = \frac{\alpha(i-1)}{i}
\]
So, the expected row sums are the same for the AIBP and IBP. Note also that
since the mechanism for drawing new dishes is the same as that for the IBP, the
expected total number of dishes drawn, which is the total number of non-zero
columns $\bm Z$ is $\alpha+\alpha/1+...+\alpha/N = \alpha H_N$, as in the IBP. 
The implication is that the effective dimensions of the IBP are preserved in the
IBP, but the allocation of dishes to customers is influenced by distance 
information.\\

\section{Simulation Study}
% Are observations close together really going to share the same dishes?
% Are distant observations going to disshare dishes?
A simulation study to compare the IBP, AIBP, and ddIBP was conducted to examine
the behavior of the AIBP \footnote{The simulation study can be accessed by
installing and loading the ``shiny" package in R, then running the line: \\
> runGitHub(`shinyTest',`luiarthur')\\\\
The code for the simulation study can be viewed at: \\
github.com/luiarthur/shinyTest}. 

\section{Comparisons between the AIBP \& ddIBP}
One obvious difference between the AIBP and ddIBP is that the pmf for any given
binary matrix can be evaluated for the AIBP, while the pmf cannot be explicitly
computed. For both the AIBP and ddIBP, expected row sums for a given parameter
$\alpha$ is $\alpha$, which is also the expected row sum for the IBP($\alpha$).
The expected number of non-zero columns in the AIBP is $\alpha H_N$, which is
also the expected number of non-zero columns in the IBP. This is because in the
AIBP, the process for each ``customer" drawing new dishes is the same as in the
IBP. This equality does not hold for the ddIBP. In general, expected column
sums are not equal to the IBP, for both the AIBP and ddIBP. However, the
expected matrix sums for both the AIBP and ddIBP are the same as that of the
IBP. The table below summarizes these findings. One implication of this result
is that the AIBP preserves the dimensions of the IBP, but redistributes
``dishes" to each customer based on proximity to other customers. The ddIBP
does not preserve the dimensions of the IBP in this manner. \\

\begin{center}
  \begin{tabular}{c|c|c}
    \hline
      Comparison & AIBP & ddIBP \\ \hline \hline
      Explicit pmf & Yes & No \\ \hline
      Expected non-zero columns equal to that of IBP & Yes & No \\ \hline
      Expected row sums equal to that of IBP & Yes & Yes \\ \hline
      Expected column sums equal to that of IBP & No & No \\ \hline
      Expected matrix sum equal to that of IBP & Yes & Yes \\ \hline
    \hline
    \caption{Table 1: Comparisons of the AIBP to the ddIBP showing how what
                      properties of the IBP they presrve.}
  \end{tabular}
\end{center}

