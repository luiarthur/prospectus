\chapter{Research Results}

\section{Notation for the AIBP}
\noindent
Let:
\begin{itemize}
  \item $\bm \sigma = (\sigma_1,..\sigma_n)$ of ${1,...n}$ gives the sequence
        in which the n items are allocated, where the $t^{th}$ item allocated
        is $\sigma_t$. 
  \item $\lambda(i,j)$ = similarity between customers $i$ and $j$.
  \item $x_i$ be the number of \textit{new} dishes that customer $i$ draws.\\ 
  \item $y_{-i} = \suml{j}{1}{i-1} x_j$, the number of existing dishes
        before customer $i$ draws new dishes.
  \item $g_i(x_i) = \ds\frac{(\alpha/i)^{x_i}\exp(-\alpha/i)}{x_i!}$, 
        which is the probability mass function for a Poisson distribution 
        with parameter $\alpha/i$ and argument $x_i$.\\
  \item $K$ = the number of non-zero columns in $\bm Z$.\\
  \item $m_{-i,k}$ = the number of customers that took dish $k$ before customer
        customer $i$.
  \item $m_{-i} = \suml{k}{1}{K}m_{-i,k}$.
  \item $h_{i,k} = \ds\frac{\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)}
        {\suml{k}{1}{y_i}\left(\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)
         \right)}$
  \item $p_{i,k} = h_{i,k} \ds\frac{m_{-i}}{i}$
  %\item $h(i) = \prodl{k}{1}{y_{-i}} 
  %      (p_{i,k})^{z_{i,k}}(1-p_{i,k})^{1-z_{i,k}}$, for $i>1$,
  %      and $1$ for $i=1$.
  \item $H_N = \suml{i}{1}{N} \ds\frac{1}{i}$.
\end{itemize}


\section{Sampling Algorithm for the AIBP}
\begin{itemize}
  \item The first customer draws an $x_1=$ Poisson($\alpha$) number of dishes. This
        is indicated by setting $z_{1,1:x_1}$ to $1$.
  \item For the next $i$ customers, for $i=1:N$, 
     \begin{itemize}
       \item if there exists previously sampled dishes, then customer $i$
             samples dish k with probability $\left(h_{i,k}\right) 
             \left(\ds\frac{m_{-i}}{i}\right)$, for $ k = 1:y_{-i}$.
       \item customer $i$ draws another Poisson($\alpha/i$) number of new dishes.
     \end{itemize}
\end{itemize}


\section{Probability Mass Function}
\begin{align*}
  \text{P}(\bm Z|\bm\sigma)
  =&\prodl{i}{1}{N} \left\{g_i(x_i) \prodl{k}{1}{y_{-i}} 
    (p_{i,k})^{z_{i,k}}(1-p_{i,k})^{1-z_{i,k}}\right\}\\
  =&\prodl{i}{1}{N} \left\{ \ds\frac{(\alpha/i)^{x_i}\exp(-\alpha/i)}{x_i!} 
    \prodl{k}{1}{y_{-i}} 
    \ds\frac{(h_{i,k}m{-i})^{z_{i,k}}(i-h_{i,k}m_{-i,k})^{1-z_{i,k}}}{i} 
    \right\} \\
  =&\ds\frac{\alpha^{\suml{i}{1}{N}x_i} \exp(-\alpha H_N)} {\prodl{i}{1}{N}x_i!} 
    \left( \prodl{i}{1}{N}i^{-x_i} \right) \times
    \\
    &\left( \prodl{i}{1}{N}\prodl{k}{1}{y_{-i}} 
    \ds\frac{\left(\hik m_{-i}\right)^{z_{i,k}}
             \left(i-\hik m_{-i}\right)^{1-z_{i,k}}}{i} \right)\\
  =&\ds\frac{\alpha^K \exp(-\alpha H_N)} {\prodl{i}{1}{N}x_i!} 
    \left( \prodl{i}{2}{N} i^{-x_i-y_{-i}}\prodl{k}{1}{y_{-i}} 
    \ds\frac{
    \left(\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)m_{-i}\right)^{z_{i,k}}
    \left(i-\sums{\sigma}{z_{-i,k}=1}{}
    \lambda(\sigma,\sigma_i)m_{-i}\right)^{1-z_{i,k}}
    }
    {\suml{k}{1}{y_{-i}}\sums{\sigma}{z_{-i,k}=1}{}\lambda(\sigma,\sigma_i)}\right)
\end{align*}


\section{Simulation Study}
A simulation study to compare the IBP, AIBP, and ddIBP was conducted to examine the
behavior of the AIBP \footnote{The simulation study can be accessed by installing 
and loading the ``shiny" package in R, then running the line: \\
> runGitHub(`shinyTest',`luiarthur')\\}. 

\section{Comparisons between the AIBP \& ddIBP}
One obvious difference between the AIBP and ddIBP is that the pmf for any given 
binary matrix can be evaluated for the AIBP, while the pmf cannot be explicitly 
computed. For both the AIBP and ddIBP, expected row sums for a given parameter
$\alpha$ is $\alpha$, which is also the expected row sum for the IBP($\alpha$). 
The expected number of non-zero columns in the AIBP is $\alpha H_N$, which is also
the expected number of non-zero columns in the IBP. This is because in the AIBP,
the process for each ``customer" drawing new dishes is the same as in the IBP. This
equality does not hold for the ddIBP. In general, expected column sums are not
equal to the IBP, for both the AIBP and ddIBP. However, the expected matrix
sums for both the AIBP and ddIBP are the same as that of the IBP. The table below 
summarizes these findings. One implication of this result is that the AIBP 
preserves the dimensions of the IBP, but redistributes ``dishes" to each customer
based on proximity to other customers. The ddIBP does not preserve the dimensions
of the IBP in this manner. \\

\begin{center}
  \begin{tabular}{c|c|c}
    \hline
      Comparison & AIBP & ddIBP \\ \hline \hline
      Explicit pmf & Yes & No \\ \hline
      Expected non-zero columns equal to that of IBP & Yes & No \\ \hline
      Expected row sums equal to that of IBP & Yes & Yes \\ \hline
      Expected column sums equal to that of IBP & No & No \\ \hline
      Expected matrix sum equal to that of IBP & Yes & Yes \\ \hline
    \hline
  \end{tabular}
\end{center}

