One challenge in latent feature modelling is that the number of latent features
has to be predetermined by the researcher or subject experts. Bayesian
nonparametrics offers methods to learn the number of latent features while
discovering the latent features themselves. The Indian buffet process (IBP)
provides a distribution over sparse binary matrices of infinite dimensions, and
can be used as a prior for feature matrices in latent feature models.  The IBP
assumes exchangeablilty, but this is not always an appropriate assumption as
data may often be inter-related. We, therefore propose a distribution, based on
the Indian buffet process, which will incorporate pairwise similarity
information between observations. \cite{ddibp} have proposed the distance
dependent Indian buffet process (dd-IBP), which indeed incorporates distance
information into the IBP. Our proposed distribution will incorporate distance
information in a different manner, using attraction information comparable to
that used in the Ewens-Pitman attraction (EPA) distribution developed by
\cite{epa}. Our goal is to propose a distribution which reduces to the IBP in
base cases, but uses attraction information to provide a distribution which has
similar properties as the EPA.
