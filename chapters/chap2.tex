% Cite:
% Blei. ddcrp
% Dahl. partition distribution including pairwise info
% IBP
% Blei. ddIBP
% Radford Neal.

% Topics: Talk about paper Here's what I am talking about, and pull the paper
% to show it's true

\chapter{Literature Review} % ~15 pages We will first review the Chinese
Before discussing how we will propose a new distribution, we will first review 
some familiar processes and distributions. We discuss the Chinese restaurant 
process (CRP), the Ewens-Pitman attraction (EPA) distribution, % Cite Dahl 
the Indian buffet process (IBP), and the distance dependent Indian
buffet process (dd-IBP). 
%formulated by Gershman. % Cite Finally, we will finally review existing McMC
%methods for sampling from the IBP.  Maybe applications also?

\subsection{The Chinese Restaurant Process}
The Chinese restaurant process (CRP) describes the dirichlet process for
creating partition distributions. The resulting distribution of the CRP is the
Ewens-Pitman distribution. I will describe the CRP below.\\

\noindent
A number of customers (observations), n, enter a Chinese restaurant one at a
time. Let $z_i$ be the table (cluster) number that each customer gets assigned
to, such that $z_i \in \left\{ 1,..,n \right\}$. Then,
\begin{equation}
  P[z_i=k|z_{1:(i-1)},\alpha] \propto 
  \begin{cases}
    \begin{array}{rll}
      n_k,    & if & k \le K\\
      \alpha, & if & k = K+1\\
    \end{array}
  \end{cases}
\end{equation}
where the normalizing constant is $\frac{1}{\alpha+i-1}$, $n_k$ is the number
of customers in table k before seating customer i, K is the number of occupied 
tables before seating customer i, and $\alpha$ is a mass parameter which 
loosely determines the number of tables that will eventually be occupied by 
the n customers. The larger $\alpha$ is, the greater the final number of 
occupied tables. The resulting partition distribution is called the 
Ewans-Pitman distribution. And if we let $\pi_n$ represent the partition
$\left\{S_1,...,S_{q_n}\right\}$, where $S_i$ is the set \\
\textbf{STOPPED RIGHT HERE!!!}
%%%%%TALK ABOUT THE PARTITION NOTATION!!!
%%%%%TALK ABOUT THE PARTITION NOTATION!!!
%%%%%TALK ABOUT THE PARTITION NOTATION!!!
%%%%%TALK ABOUT THE PARTITION NOTATION!!!

\noindent
Gibbs samplers to generate this process has been studied extensively. 
An valuable list and comprison of algorithms to draw from the posterior with 
a CRP prior was compiled by Neal (2000).

\subsection{Distance Dependent Chinese Restaurant Process}
Researchers such as Blei \& Frazier have extended CRP to include 
distance information. The distribution they have proposed is the distance
dependent Chinese restaurant process (ddCRP). \\

\noindent
Given a distance matrix, D, and a mass parameter, $\alpha$, the probability
of customer i being ``assigned" to sit with customer j is 
\begin{equation}
  P[c_i=j|D,\alpha] \propto 
  \begin{cases}
    \begin{array}{rll}
      f(d_{ij}), & if & j \ne i\\
      \alpha,    & if & i=j
    \end{array}  
  \end{cases}
\end{equation}
where $f(d_{ij})$ is a decay function with the properties: (1) $f(\cdot) \ge 0$,
(2) $f(\infty) = 0$, and $f(\cdot)$ is non-decreasing. The normalizing constant 
is again $\frac{1}{\alpha+i-1}$. Customers assigned to sit together at a table
form a cluster. The resulting distribution is a partition distribution that
includes pairwise distance information. However, no closed form probability 
mass function for the resulting partition distribution can be written for 
Blei's proposed ddcrp. Blei describes a Gibbs sampler to sample from the 
ddcrp and its posterior.

\subsection{Ewens-Pittman Attraction Distribution}


\subsection{The Indian buffet process}
The Indian buffet process is a distribution on infinite sparse binary matrices.
(i.e. matrices with finite number of rows and infinite number of columns.) The
IBP is a multivariate extension of the Chinese restaurant process. For an N x K
matrix, Z, that follows an IBP($\alpha$) distribution, element $z_{ik}$ is 1 if
observation (customer) i possess feature (dish) k, and 0 otherwise.  And
$\alpha$ is a parameter that determines the sparsity of the matrix. The larger
$\alpha$ is, the more likely Z will be sparse. The process can be generated as
follows: \\

\noindent
N customers enter a buffet one after another. The buffet line contains an
infinite number of dishes. The first customer takes the first Poisson($\alpha$)
number of dishes. The $i^{th}$ customer then takes previously sampled dishes
with probability proportional to their popularity, serving himself with
probability $\frac{m_k}{i}$, where $m_k$ is the number of people that had
previously taken dish k. After customer i has sampled all the previously sampled
dishes, he samples Poisson($\frac{\alpha}{i}$) new dishes. The probability of 
any matrix being generated by this process is
\begin{equation}
  P(\bm{Z}) = \frac{\alpha^{K_+}}{\prodl{i}{1}{N} K_1^{(i)}!} 
              exp\{-\alpha H_N\}\prodl{k}{1}{K_+}
              \frac{(N-m_k)!(m_k-1)!}{N!},
\end{equation}
where $H_N$ is the harmonic number, $\suml{i}{1}{N}\ds\frac{1}{i}$, $K_+$ is the 
number of non-zero columns in $\bm Z$, and $K_1^{(i)}$ is the number of new dishes
sampled by customer i. %cite the derivation

\subsection{Gibbs sampler}
Here we describe how to implement a Gibbs sampler to draw from the IBP. 
To sample from Z $\sim$ IBP($\alpha$):

\begin{enumerate}
  \item Start with an arbitrary binary matrix
  \item For each row, i,
  \begin{enumerate}
    \item For each column, k,
    \item if $m_{-i,k}$ = 0, delete column k. Otherwise,
    \item set $z_{ik}$ to 0
    \item set $z_{ik}$ to 1 with probability $P(z_{ik}=1|\bm{z_{-i,k}}) = \frac{m_{-i,k}}{i}$
    \item at the end of row i, add Poisson($\ds\frac{\alpha}{N}$) columns of 1's
  \end{enumerate}
  \item iterate step 2 a large number of times
\end{enumerate}
We can likewise incorporate this Gibbs sampler to sample from the posterior distribution P($\bm{Z|X}$)
where $\bm Z \sim IBP(\alpha)$ by sampling from the complete conditional

\begin{equation}
  P(z_{ik}=1|\bm{Z_{-(ik)},X})  \propto p(\bm{X|Z}) P(z_{ik}=1|\bm{Z_{-(ik)}}).
\end{equation}

%\subsection{Distance Dependent Indian Buffet Process}
%Currently, there exists a distance dependent Indian buffet process (dd-IBP), 
%constructed by Gershman, Frazier, and Blei (2012). We wish to create a model 
%similar to it. We will first review the ddIBP.

\subsection{Random Partition Distribution Indexed by Pairwise Information}




\subsection{Applications}
%The applications of the Indian buffet distribution include defining priors

% What is the IBP? # Done
% Gibbs sampler to draw from prior # Done
% Gibbs sampler to draw from posterior # Done
% What is it used for?
% Where are the applications?
% Why would it be useful to include distance information?
% ddIBP?

