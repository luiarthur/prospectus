% Topics: Talk about paper Here's what I am talking about, and pull the paper
% to show it's true

\chapter{Literature Review} % ~15 pages
In this section, we lay the groundwork for out proposed distribution by reviewing
important processes and distributions. We will discuss the Chinese restaurant
process, the distance dependent Chinese restaurant process \citep{ddcrp}, the
Ewens-Pitman attraction distribution \citep{epa}, the Indian buffet process
\citep{ibp}, and the distance dependent Indian buffet process \citep{ddibp}.
%, and the distance dependent Indian buffet process (dd-IBP). 
%formulated by Gershman. % Cite Finally, we will finally review existing McMC
%methods for sampling from the IBP.  Maybe applications also?

\section{The Chinese Restaurant Process}
The Chinese restaurant process (CRP) describes the Dirichlet process, a process
that creates partition distributions. The partition distribution generated by
the CRP is the Ewens-Pitman distribution. The CRP can be described as follows.
A number of customers (observations), $n$, enter a Chinese restaurant to be
seated one at a time. Let $z_i$ be the table (cluster) number that each
customer gets assigned to, such that $z_i \in \left\{ 1,..,n \right\}$, for $i
= 1,...,n$. Let $z_{1:(i-1)}$ denote the vector of table numbers that customers
$1,...,i-1$ are assigned to. Then,
\begin{equation}
  P(z_i=k|z_{1:(i-1)},\alpha) \propto 
  \begin{cases}
    \begin{array}{rll}
      n_k,    & \text{if} & k \le K\\
      \alpha, & \text{if} & k = K+1\\
    \end{array}
  \end{cases}
\end{equation}
where the normalizing constant is ${(\alpha+i-1)}^{-1}$, $n_k$ is the number of
customers in table k before seating customer $i$, $K$ is the number of occupied
tables before seating customer $i$, and $\alpha$ is a mass parameter which
determines the number of tables that will eventually be occupied by the $n$
customers. The larger $\alpha$ is, the greater the final number of occupied
tables. The resulting partition distribution is called the Ewans-Pitman
distribution. Let $\pi_n$ represent the partition
$\left\{S_1,...,S_{q_n}\right\}$, where $q_n \in \lb1,...,n\rb$ is the number
of partitions and each $S_i$ is a set representing a cluster or customers
seated at table $i$, $\pi_n$ will have the properties: (1) $S_i \cap S_j = \o$
for $i \ne j$ (sets are mutually exclusive), (2) $\underset{S \in \pi_n}{\cap}
S = \lb 1,...,n \rb$ (sets are exhaustive), and (3) $S_i \ne \o$, $\forall i
\in \lb 1,...,q_n\rb$ (no sets are empty).  The probability mass function for
the Ewens-Pitman distribution is
\begin{equation}
  P(\pi_n) = \underset{S \in \pi_n}{\prod} 
             \frac{\alpha\Gamma(|S|)}{\alpha^{(n)}}
\end{equation}

\subsection{Gibbs Sampler for CRP}
Gibbs samplers to generate this process have been studied extensively.  A
valuable list and comprison of algorithms to draw from the posterior with a CRP
prior was compiled by Neal \citep{neal}. An implementation of some of these
algorithms in \textbf{R} and \textbf{Scala} can be found at my github account:
https://github.com/luiarthur/auxGibbs/tree/master/scala\\

%\noindent
%I will describe Algorithm 8 from Neals paper. \\
%Let the state


\section{Distance Dependent Chinese Restaurant Process}
Researchers such as Blei \& Frazier have extended CRP to include distance
information. The distribution they have proposed is the distance dependent
Chinese restaurant process (ddCRP). Given a distance matrix D and a mass
parameter $\alpha$, the probability of customer $i$ being ``assigned" to sit
with customer $j$ is 
\begin{equation}
  P(c_i=j|D,\alpha) \propto 
  \begin{cases}
    \begin{array}{rll}
      f(d_{ij}), & \text{if} & j \ne i\\
      \alpha,    & \text{if} & i=j
    \end{array}  
  \end{cases}
\end{equation}
where $f(d_{ij})$ is a decay function with the properties: (1) $f(\cdot) \ge
0$, (2) $f(\infty) = 0$, and $f(\cdot)$ is non-decreasing. The normalizing
constant is again $(\alpha+i-1)^{-1}$. Customers assigned to sit together at a
table form a cluster. The resulting distribution is a partition distribution
that includes pairwise distance information. However, there is no closed form
probability mass function for the resulting partition distribution for Blei's.
Blei describes a Gibbs sampler to sample from the ddcrp and its posterior. The
dCRP is available due to the complex combinations of enumerating all
assignments that map to a particular partition.

\section{Ewens-Pittman Attraction Distribution}
The Ewens-Pitmann Attraction (EPA) distribution is another partition
distribution that incorporates pairwise distance information \cite{epa}.
Before introducing the probability mass function, I will review some notation.
Let $\pi_n$ be a discrete partition distribution, as defined in the previous
section. Let the permutation $\bm \sigma = (\sigma_1,...,\sigma_n)$ be the
order in which each item is allocated, where the $t^{th}$ item to be allocated
is $\sigma_t$. This is not necessarily the order of the n items in the dataset.
In addition, at time $t > 1$, let $\pi(\sigma_{1},...,\sigma_{t-1})$ represent
the current partition created from allocating $\sigma_{1},...,\sigma_{t-1}$.
Note that the complete partition $\pi_n = \pi(\sigma_{1},...,\sigma_{n})$.\\

\noindent
The EPA distribution incorporates pairwise distance information. A possible
metric for measuring distance is the Euclidean metric. Let $d_{ij}$ denote the
distance between two items, $i$ and $j$. Let the function $\lambda(i,j)$ represent
the similarity of items $i$ and $j$ (i.e. how ``close" two items are, where a larger
value indicates that the two items are closer together). A large class of similarity
functions can be represented as a function of the distance information. That is
$\lambda(i,j)$ can be written as $f(d_{ij})$ in many cases, where $f(\cdot)$ is a
non-increasing function. For instance, $\lambda(i,j)$ can be ${d_{ij}}^{-\tau}$,
where $\tau>0$ and is called the \textit{temperature} and can dampen or
accentuate the effect of distance.\\

\noindent
The EPA distribution is also defined by a mass parameter, $\alpha > 0$, and a
discount parameter, $\delta \in [0,1)$. The probability mass function for a
partition distribution $\pi_n$ following the EPA distribution can be defined as
follows:
\begin{equation}
  p(\pi_n|\alpha,\delta,\lambda,\bm\sigma)=
    \prodl{i}{1}{n}p_t(\alpha,\delta,\lambda,\pi(\sigma_1,...,\sigma_{t-1}))
\end{equation}
where $p_t(\alpha,\delta,\lambda,\pi(\sigma_1,...,\sigma_{t-1}))$ is defined as:
\begin{equation}
  P(\sigma \in S | \alpha,\delta,\lambda,\pi(\sigma_1,...,\sigma_{t-1}))=
  \begin{cases}
    \begin{array}{ll}
      \ds\frac{t-1-\delta q_{t-1}}{\alpha+t-1} \cdot 
        \frac{\sum_{\sigma_s \in S}\lambda(\sigma_t,\sigma_s)}
        {\sum_{s=1}^{t-1}\lambda(\sigma_t,\sigma_s)} & 
        \text{for }S \in \pi(\sigma_1,...,\sigma_{t-1})\\
      \ds\frac{\alpha+\delta q_{t-1}}{\alpha+t-1} &  \text{for }S  = \o \\
    \end{array}
  \end{cases}
\end{equation}
Note that the ratio of sums in (2.5) represents the proportion of \textit{total
attraction} of item $\sigma_t$ to the items allocated to subset $S$.


\section{The Indian Buffet Process}
One key problem in recovering the latent structure responsible for generating
observed data is determining the number of latent features. The Indian Buffet
process (IBP) provides a flexible distribution for sparse binary matrices with
infinite dimensions (i.e. finite number of rows, and infinite number of columns).
When used as a prior distribution in a latent feature model, the IBP can
learn the number of latent features generating the observations because it can
draw binary matrices which have a potentially infinite number of columns. I will
use the IBP as a prior distribution in a Gaussian latent feature model to
recover the latent structures generating the observations \cite{ibp}.\\

\noindent
The IBP is a distribution for sparse binary matrices with finite number of rows
and potentially infinite number of columns. The process of generating a
realization from the IBP can be described by an analogy involving Indian buffet
restaurants.\\

\noindent
Let $Z$ be an $N \times \infty$ binary matrix. Each row in $Z$ represents a
customer which enters an Indian buffet and each column represents a dish in the
buffet. Customers enter the restaurant one after another. The first customer
samples an $r=$Poisson$(\alpha)$ number of dishes, where $\alpha > 0$ is a mass
parameter which influences the final number of sampled dishes. This is
indicated in by setting the first r columns of the first row in $Z$ to be $1$.
The other values in the row are set to $0$. Each subsequent customer samples
each previously sampled dish with probability proportional to its popularity.
That is, the next customer samples dish $k$ with probability $m_k/i$,
where $m_k$ is the number of customers that sampled dish $k$, and $i$ is the
current customer number (or row number in $Z$). Each customer also samples an
additional Poisson$(\alpha/i)$ number of new dishes. Once all the $N$ customers
have gone through this process, the resulting $Z$ matrix will be a draw from
the Indian buffet process with mass parameter $\alpha$. In other words, $Z \sim
\text{IBP}(\alpha)$. Note that $\alpha \propto K_+$, where $K_+$ is the final
number of sampled dishes (occupied columns). Figure 2.1 shows a draw from an
IBP(10) with 50 rows. The white squares are 1, indicating that a dish was
taken; black squares are 0, indicating that a dish was not taken. \\
\beginmyfig
  % Reverse Colors
  \caption{IBP($N=50$, $\alpha=10$)}
  \includegraphics{images/ibp.pdf}
\endmyfig

\noindent
The probability of any particular matrix produced from this process is
\begin{equation}
  P(\bm{Z}) = \frac{\alpha^{K_+}}{\prodl{i}{1}{N} {K_1}^{(i)}!} 
              \exp\{-\alpha H_N\}\prodl{k}{1}{K_+}
              \frac{(N-m_k)!(m_k-1)!}{N!},
\end{equation}
where $H_N$ is the harmonic number, $\suml{i}{1}{N}\ds\frac{1}{i}$, $K_+$ is
the number of non-zero columns in $\bm Z$, $m_k$ is the $k^{th}$ column sum of
$\bm Z$, and $K_1^{(i)}$ is the ``number of new dishes" sampled by customer $i$.\\

\subsection{Gibbs Sampler for Indian Buffet Process}
One way to get a draw from the IBP($\alpha$) is to simulate the process
according to the description above. Another way is to implement a Gibbs
sampler. We can use a Gibbs sampler in MCMC to obtain the posterior
distribution, given a likelihood. We can implement a Gibbs sampler as follows:

\begin{enumerate}
  \item Start with an arbitrary binary matrix of $N$ rows
  \item For each row, $i$,
  \begin{enumerate}
    \item For each column, $k$,
    \item if $m_{-i,k}$ = $0$, delete column $k$. Otherwise,
    \item set $z_{ik}$ to $0$
    \item set $z_{ik}$ to $1$ with probability $P(z_{ik}=1|\bm{z_{-i,k}}) = \frac{m_{-i,k}}{i}$
    \item at the end of row $i$, add Poisson($\ds\frac{\alpha}{N}$) columns of $1$'s
  \end{enumerate}
  \item iterate step 2 a large number of times
\end{enumerate}
We can likewise incorporate this Gibbs sampler to sample from the posterior distribution P($\bm{Z|X}$)
where $\bm Z \sim \text{IBP}(\alpha)$ by sampling from the complete conditional
\begin{equation}
  P(z_{ik}=1|\bm{Z_{-(ik)},X})  \propto p(\bm{X|Z}) P(z_{ik}=1|\bm{Z_{-(ik)}}).
\end{equation}\\


\noindent
The parameter $\alpha$ is often unknown, so it should be modeled. Note that a
conjugate prior for $\alpha$ is a Gamma distribution. Using a Gamma
distribution is appropriate since $\alpha$ is positive.
\[
  \begin{array}{rcl}
    \bm Z|\alpha & \sim & \text{IBP}(\alpha)\\
          \alpha & \sim & \text{Gamma}(a,b), \text{where $b$ is the scale parameter}\\
    & & \\
    p(\alpha|\bm Z) & \propto & p(\bm Z|\alpha) p(\alpha)\\
    p(\alpha|\bm Z) & \propto & \alpha^{K_+} e^{-\alpha H_N}  
                                \alpha^{a-1} e^{-\alpha/b}\\
    p(\alpha|\bm Z) & \propto & \alpha^{a+K_+-1} e^{-\alpha(1/b+H_N)}\\
  \end{array}
\]
\begin{equation}
  \alpha|\bm Z  \sim  \text{Gamma}(a+K_+, (1/b+H_N)^{-1})
\end{equation}

% IBP Implementation.
\subsection{Example: Linear-Gaussian Latent Feature Model with Binary Features}
Suppose, we observe an $N \times D$ matrix $\bm X$, and we believe
\[
  % Who do we attribute this to?
  \bm X = \bm{ZA} + \bm E,
\]
where $\bm Z|\alpha \sim IBP(\alpha)$, 
$\bm A \sim MVN(\bm 0,{\sigma_A}^2\textbf{I})$, 
$\bm E \sim MVN(\bm 0,{\sigma_X}^2\textbf{I})$, and
$\alpha \sim \text{Gamma}(a,b)$,
%$\sigma_A=1$, and $\sigma_X=.5$.\\

\noindent
It can be shown that
\begin{equation}\begin{split}  
  p(\bm{X|Z}) = \ds\frac{1}{(2\pi)^{ND/2}\sigma_X^{(N-K)D}\sigma_A^{KD}
                            |\bm Z^T \bm Z+({\frac{\sigma_X}{\sigma_A}})^2
                            \textbf{I}|^{D/2}}\\
                \exp\{-\frac{1}{2\sigma_X^2}tr(\bm X^T
                (\textbf{I}-\bm Z(\bm Z^T\bm Z+({\frac{\sigma_X}{\sigma_A}})^2
                            \textbf{I})^{-1}\bm Z))
                \bm X\}            
\end{split}\end{equation}
Now, we can use equation (2) to implement a Gibbs sampler to draw from the 
posterior $\bm{Z|X},\alpha$.

\subsection{Data \& Results}
Each of ten students created a $6 \times 6$ binary image. To the
image, Gaussian noise (mean=0, variance=.25) was added to each cell of the
binary image. Gaussian noise was added to the same image to generate 10 $6
\times 6$ images. These images were turned into $1 \times 36$ row vectors. All
these vectorized images were stacked together to form one large $100 \times 36$
matrix. (Note that I did not know the design of these images before hand. The
images could be letters, numbers, interesting patterns, etc.)
Figure 2.2 displays the data from the ten students.\\

\beginmyfig
  \vspace{-5mm}
  \caption{Data From Ten Students}
  \vspace{-2mm}
  \includegraphics{images/Y.pdf}
  \vspace{-15mm}
\endmyfig

\noindent
A Gibbs sampler was implemented to retrieve posterior distributions for $\bm
Z$, $\bm A$, and $\alpha$. The mass parameter$\alpha$ was initially set to $1$,
and the posterior for $\alpha$ was obtained by equation (3) with prior
distribution $Gamma(3,2)$.  The parameters were chosen such that $\alpha$ was
centered at 6 and had a variance of 12, as I believed I may have many latent
features, but my uncertainty was high.  Equation(2) was used to retrieve the
posterior distribution for $\bm Z$ (a collection of binary matrices). After
5000 iterations, the trace plot for the number of columns in the binary
matrices drawn were plotted (See Figure 2.3). Diagnostics for a cell by cell
trace plot could also be plotted, but may be too difficult to analyze as the
dimensions of the matrices are changing, and the matrices are large. The
execution time was about 4 hours.
\beginmyfig
  \caption{}
  \includegraphics{images/traceplot.pdf}
  \vspace{-15mm}
\endmyfig
\beginmyfig
  \caption{}
  \includegraphics{images/postAlpha.pdf}
\endmyfig
\noindent
The number of columns in $\bm Z$ appears to have converged to 9. This means
that the number of latent features discovered appears to be 9. This is
reasonable as the image $\bm X$ is comprised of 10 students' images. A burn-in
of the first 1000 draws were removed. Then, the 4000 $\bm Z$ matrices were
superimposed, summed element by element, and divided by 4000. Cells that had
values $> .9$ were set to 1; and 0 otherwise. This is not necessary, but this
removes the columns that were not likely to exist. To elaborate, from the trace
plot (Figure 2.3), we see that after burn-in, there is one instance where the
number of columns in $\bm Z$ was $10$.  One instance out of 4000 is not
significantly large enough to say that \textit{that} latent feature is 
generating the observed data. So, the tenth column was removed. The resulting
matrix, I will call the posterior mean for $\bm Z$.  The trace plot for
$\alpha$ (Figure 2.4) shows that $\alpha$ appears to have converged, with mean
$=2.08$ and variance $=.359$.  Figure 2.5 shows the posterior mean for $\bm Z$.
We can interpret the matrix in the following way. All the observations are 
being generated by the first column (feature). The $11^{th}$ through $20^{th}$
observations in $\bm X$ are being generated by the second column, etc.
The posterior mean for $\bm A$ calculated as $E[\bm{A|X,Z}] = (\bm Z^T\bm Z +
\frac{\sigma_X^2}{\sigma_A^2} \textbf{I})^{-1}\bm Z^T \bm X$, and is shown
in Figure 2.6 and Figure 2.7. Figure 2.6 shows the matrix in a $10 \times 36$ form;
Figure 2.7 shows the matrix in $6 \times 6$ form. That is, each row in $\bm A$
were back-transformed into its matrix form.\\

\beginmyfig
  \caption{}
  \includegraphics{images/postZ.pdf}
  \vspace{-10mm}
\endmyfig

\beginmyfig
  \caption{}
  \includegraphics{images/postA.pdf}
\endmyfig
\beginmyfig
  \caption{The latent features turned back into $6 \times 6$ images.}
  %\includegraphics[height=1\textwidth]{images/postA66.pdf}
  \includegraphics{images/postA66.pdf}
\endmyfig

\noindent
Now, we can predict the latent features generating each observation by
multiplying the posterior mean of $\bm Z$ by the posterior mean of $\bm A$. We
will call this matrix the posterior mean of $\bm {ZA}$. Each row in this matrix
will reveal the latent structures generating the observation in the respective
row of $\bm X$. Figure 2.8 shows the latent structure learned from the data. 
For each observation, the data and latent structures are layed side by side
for a visual comparison. The features learned were similar to the images
created by the ten students.


\begin{figure}\begin{center}
  \caption{The latent features for each student}
  %\includegraphics[height=1\textwidth]{images/postA66.pdf}
  \includegraphics[height=1\textwidth]{images/postFriends.pdf}
\end{center}\end{figure}


\section{Distance Dependent Indian Buffet Process}
Currently, there exists a distance dependent Indian buffet process (ddIBP), 
constructed by \cite{ddibp}. We wish to create a model 
similar to it. We will first review the ddIBP. \\

\noindent
In the ddIBP, following the same analogy as in the IBP, two customers that are
``closer" together in a given distance metric are more likely to share the same
dishes (features). This is different from the IBP as new customers do not take
new dishes according to their popularity. Terminology and notation for the ddIBP
will here be introduced:

\begin{enumerate}
  \item Dishes (columns of $\bm Z$) are identified by the natural numbers 
        $\mathbb{N}$

  \item $\mathcal{K}_i$: The set of dishes owned by customer $i$
  \begin{itemize}
    \item $\mathcal{K}_i \subset \mathbb{N}$
    \item $\mathcal{K}_i \cap \mathcal{K}_j = \o$, for $i \ne j$
    \item $\lambda_i = |\mathcal{K}_i|$
    \item $K = \suml{i}{1}{N} \lambda_i$ 
    \item $K_{-i} = \cup_{j \ne i}K_j$
  \end{itemize}

  \item $\bm C$: The N $\times $ K connectivity matrix, which indicates how 
                 customers are ``linked" to each other by dishes
  \begin{itemize}
    \item $c_{ik} = j \implies$ customer $i$ connects to customer $j$ through
          dish $k$
  \end{itemize}

  \item $\bm{c^*}$: The ownership vector, $\bm{c_k^*} \in \{1,...,N\}$ indicates
                   the owner of dish $k$
  \begin{itemize}
    \item $\bm{c_k^*}=i \implies k \in \mathcal{K}_i$
  \end{itemize}

  \item $\bm D$: The N $\times$ N distance matrix, where $d_{ij}$ indicates the 
                 distance of customer $j$ from customer $i$
  
  \item $f: \mathbb{R} \rightarrow [0,1]$: The decay function, $f$ maps the 
                                           distance between customers to unity.
                                           The decay function has the properties:
                                           \begin{itemize}
                                             \item $f(0) = 1$
                                             \item $f(\infty) = 0$
                                           \end{itemize}

  \item $\bm A$: The N $\times$ N normalized proximity matrix is defined as
                 $a_{ij} = f(d_{ij}) / h_i$, where $h_i = \suml{j}{1}{N} f(d_{ij})$
\end{enumerate}

\noindent
To generate an observation from the ddIBP with mass parameter $\alpha$, decay
function $f$, and distance matrix $\bm D$ for N observations, we use the following
algorithm:

\begin{enumerate}
  \item Set $\lambda_0:=0$

  \item For $i$ = $1:N$
    \begin{itemize}
      \item draw $\lambda_i \sim \text{Poisson}(\alpha / h_i) $
      \item set $\mathcal{K}_i:= 
            \{\suml{j}{0}{i-1} \lambda_j,...,\suml{j}{0}{i-1} \lambda_j+\lambda_i \}$
      \item for each $k \in \mathcal{K}_i$, set $\bm{c_k^*}:=i$
    \end{itemize}

  \item For $i=1:N$
    \begin{itemize}
      \item for $k = 1:K$, assign customer $i$ to connect to customer $j$ by 
            dish $k$ with probability P$(c_{ik}=j|\bm D,f) = a_{ij}$, where
            $j = 1,...,N$ 
    \end{itemize}

  \item Generate $\bm Z$ matrix: 
    \begin{itemize}
      \item for each customer $i$
      \begin{itemize}
        \item for each dish $k$,
              if dish $k$ is reachable by customer $i$ through other customers,
              or if customer $i$ owns the dish, then $z_{ik}:=0$
      \end{itemize}
    \end{itemize}
\end{enumerate}

\noindent
The feature matrix, $\bm Z$ is computed deterministically from the connectivity 
matrix $\bm C$ and ownership vector $\bm{c^*}$. The joint p.m.f for $\bm C$ and 
$\bm c^*$ can be computed as:
\begin{equation}
  P(\bm C, \bm{c^*}|\bm D,\alpha,f) = P(\bm{c^*|\alpha}) P(\bm C|\bm{c^*},\bm D,f)
\end{equation}
where $P(\bm{c^*|\alpha}) = \prodl{i}{1}{N} P(\lambda_i|\alpha),$ and 
$P(\bm C|\bm{c^*},\bm D,f) = \prodl{i}{1}{N}\prodl{k}{1}{K}a_{ic_{ik}}$.\\

\noindent
The p.m.f for $\bm Z$ can then be computed as:
\begin{equation}
  P(\bm{Z|D},\alpha,f) = \suml{(\bm{c^*,C}):\phi(\bm{c^*,C})}{\bm Z}{} 
                         P(\bm C, \bm{c^*}|\bm D,\alpha,f)
\end{equation}
where $\phi$ is a many to one function that maps the connectivity matrix and
ownership vector to the appropriate $\bm Z$ matrix.


%The applications of the Indian buffet distribution include defining priors

% What is the IBP? # Done
% Gibbs sampler to draw from prior # Done
% Gibbs sampler to draw from posterior # Done
% What is it used for?
% Where are the applications?
% Why would it be useful to include distance information?
% ddIBP?

