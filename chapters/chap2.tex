% Cite:
% crp          (done)
% Radford Neal (done)
% Blei. ddcrp  (done)
% IBP          (done)
% Dahl. partition distribution including pairwise info
% Blei. ddIBP

% Topics: Talk about paper Here's what I am talking about, and pull the paper
% to show it's true

\chapter{Literature Review} % ~15 pages
In this section, we lay the groundwork for out proposed distribution by reviewing
important processes and distributions. We will discuss the Chinese restaurant
process, the distance dependent Chinese restaurant process \citep{ddcrp}, the
Ewens-Pitman attraction distribution \citep{epa}, the Indian buffet process
\citep{ibp}, and the distance dependent Indian buffet process \citep{ddibp}.
%, and the distance dependent Indian buffet process (dd-IBP). 
%formulated by Gershman. % Cite Finally, we will finally review existing McMC
%methods for sampling from the IBP.  Maybe applications also?

\section{The Chinese Restaurant Process}
The Chinese restaurant process (CRP) describes the Dirichlet process, a process
that creates partition distributions. The partition distribution generated by the 
CRP is the Ewens-Pitman distribution. The CRP can be described as follows.\\

\noindent
A number of customers (observations), $n$, enter a Chinese restaurant to be
seated one at a time. Let $z_i$ be the table (cluster) number that each
customer gets assigned to, such that $z_i \in \left\{ 1,..,n \right\}$, for
$i = 1,...,n$. Then,
\begin{equation}
  P(z_i=k|z_{1:(i-1)},\alpha) \propto 
  \begin{cases}
    \begin{array}{rll}
      n_k,    & \text{if} & k \le K\\
      \alpha, & \text{if} & k = K+1\\
    \end{array}
  \end{cases}
\end{equation}
where the normalizing constant is ${(\alpha+i-1)}^{-1}$, $n_k$ is the number of
customers in table k before seating customer $i$, K is the number of occupied
tables before seating customer $i$, and $\alpha$ is a mass parameter which
determines the number of tables that will eventually be occupied by the $n$
customers. The larger $\alpha$ is, the greater the final number of occupied
tables. The resulting partition distribution is called the Ewans-Pitman
distribution. And if we let $\pi_n$ represent the partition
$\left\{S_1,...,S_{q_n}\right\}$, where $q_n \in \lb1,...,n\rb$ and represents
the number of partitions, and each $S_i$ is a set representing a cluster or
customers seated at table $i$, $\pi_n$ will have the properties: (1) $S_i \cap
S_j = \o$ for $i \ne j$ (sets are mutually exclusive), (2) $\underset{S \in
\pi_n}{\cap} S = \lb 1,...,n \rb$ (sets are exhaustive), and (3) $S_i \ne \o$,
$\forall i \in \lb 1,...,q_n\rb$ (no sets are empty).  The probability mass
function for this partition distribution, the Ewens-Pitman distribution, is
\begin{equation}
  P(\pi_n) = \underset{S \in \pi_n}{\prod} 
             \frac{\alpha\Gamma(|S|)}{\alpha^{(n)}}
\end{equation}

\subsection{Gibbs Sampler for CRP}
Gibbs samplers to generate this process have been studied extensively.  A
valuable list and comprison of algorithms to draw from the posterior with a CRP
prior was compiled by Neal \citep{neal}. An implementation of some of these
algorithms in \textbf{R} and \textbf{Scala} can be found at my github account:
https://github.com/luiarthur/auxGibbs/tree/master/scala\\

%\noindent
%I will describe Algorithm 8 from Neals paper. \\
%Let the state


\section{Distance Dependent Chinese Restaurant Process}
Researchers such as Blei \& Frazier have extended CRP to include 
distance information. The distribution they have proposed is the distance
dependent Chinese restaurant process (ddCRP). \\

\noindent
Given a distance matrix, D, and a mass parameter, $\alpha$, the probability
of customer $i$ being ``assigned" to sit with customer $j$ is 
\begin{equation}
  P(c_i=j|D,\alpha) \propto 
  \begin{cases}
    \begin{array}{rll}
      f(d_{ij}), & \text{if} & j \ne i\\
      \alpha,    & \text{if} & i=j
    \end{array}  
  \end{cases}
\end{equation}
where $f(d_{ij})$ is a decay function with the properties: (1) $f(\cdot) \ge 0$,
(2) $f(\infty) = 0$, and $f(\cdot)$ is non-decreasing. The normalizing constant 
is again $(\alpha+i-1)^{-1}$. Customers assigned to sit together at a table
form a cluster. The resulting distribution is a partition distribution that
includes pairwise distance information. However, no closed form probability 
mass function for the resulting partition distribution has been written for 
Blei's proposed ddcrp. Blei describes a Gibbs sampler to sample from the 
ddcrp and its posterior.

\section{Ewens-Pittman Attraction Distribution}
The Ewens-Pitmann Attraction (EPA) distribution is another partition
distribution that incorporates pairwise distance information \cite{epa}.
Before introducing the probability mass function, I will review some
notation.\\

\noindent
Let $\pi_n$ be a discrete partition distribution, as defined in the previous
section. Let the permutation $\bm \sigma = (\sigma_1,...,\sigma_n)$ be the order
in which each item is allocated, where the $t^{th}$ item to be allocated is
$\sigma_t$. This is not necessarily the order of the n items in the dataset. In
addition, at time $t > 1$, let $\pi(\sigma_{1},...,\sigma_{t-1})$ represent the
current partition created from allocating $\sigma_{1},...,\sigma_{t-1}$. Note
that the complete partition $\pi_n = \pi(\sigma_{1},...,\sigma_{n})$.\\

\noindent
The EPA distribution incorporates pairwise distance information. A possible
metric for measuring distance is the Euclidean metric. Let $d_{ij}$ denote the
distance between two items, $i$ and $j$. Let the function $\lambda(i,j)$ represent
the similarity of items $i$ and $j$ (i.e. how ``close" two items are, where a larger
value indicates that the two items are closer together). A large class of similarity
functions can be represented as a function of the distance information. That is
$\lambda(i,j)$ can be written as $f(d_{ij})$ in many cases, where $f(\cdot)$ is a
non-increasing function. For instance, $\lambda(i,j)$ can be ${d_{ij}}^{-\tau}$,
where $\tau>0$ and is called the \textit{temperature} and can dampen or
accentuate the effect of distance.\\

\noindent
The EPA distribution is also defined by a mass parameter, $\alpha > 0$, and a
discount parameter, $\delta \in [0,1)$. The probability mass function for a
partition distribution $\pi_n$ following the EPA distribution can be defined as
follows:
\begin{equation}
  p(\pi_n|\alpha,\delta,\lambda,\bm\sigma)=
    \prodl{i}{1}{n}p_t(\alpha,\delta,\lambda,\pi(\sigma_1,...,\sigma_{t-1}))
\end{equation}
where $p_t(\alpha,\delta,\lambda,\pi(\sigma_1,...,\sigma_{t-1}))$ is defined as:
\begin{equation}
  P(\sigma \in S | \alpha,\delta,\lambda,\pi(\sigma_1,...,\sigma_{t-1}))=
  \begin{cases}
    \begin{array}{ll}
      \ds\frac{t-1-\delta q_{t-1}}{\alpha+t-1} \cdot 
        \frac{\sum_{\sigma_s \in S}\lambda(\sigma_t,\sigma_s)}
        {\sum_{s=1}^{t-1}\lambda(\sigma_t,\sigma_s)} & 
        \text{for }S \in \pi(\sigma_1,...,\sigma_{t-1})\\
      \ds\frac{\alpha+\delta q_{t-1}}{\alpha+t-1} &  \text{for }S  = \o \\
    \end{array}
  \end{cases}
\end{equation}
Note that the ratio of sums in (2.5) represents the proportion of \textit{total
attraction} of item $\sigma_t$ to the items allocated to subset $S$.


\section{The Indian buffet process}
One key problem in recovering the latent structure responsible for generating
observed data is determining the number of latent features. The Indian Buffet
process (IBP) provides a flexible distribution for sparse binary matrices with
infinite dimensions (i.e. finite number of rows, and infinite number of columns).
When used as a prior distribution in a latent feature model, the IBP can
learn the number of latent features generating the observations because it can
draw binary matrices which have a potentially infinite number of columns. I will
use the IBP as a prior distribution in a Gaussian latent feature model to
recover the latent structures generating the observations \cite{ibp}.\\

\noindent
The IBP is a distribution for sparse binary matrices with finite number of rows
and potentially infinite number of columns. The process of generating a
realization from the IBP can be described by an analogy involving Indian buffet
restaurants.\\

\noindent
Let $Z$ be an $N \times \infty$ binary matrix. Each row in $Z$ represents a
customer which enters an Indian buffet and each column represents a dish in the
buffet. Customers enter the restaurant one after another. The first customer
samples an $r=$Poisson$(\alpha)$ number of dishes, where $\alpha > 0$ is a mass
parameter which influences the final number of sampled dishes. This is
indicated in by setting the first r columns of the first row in $Z$ to be $1$.
The other values in the row are set to $0$. Each subsequent customer samples
each previously sampled dish with probability proportional to its popularity.
That is, the next customer samples dish $k$ with probability $\frac{m_k}{i}$,
where $m_k$ is the number of customers that sampled dish $k$, and $i$ is the
current customer number (or row number in $Z$). Each customer also samples an
additional Poisson$(\alpha/i)$ number of new dishes. Once all the $N$ customers
have gone through this process, the resulting $Z$ matrix will be a draw from
the Indian buffet process with mass parameter $\alpha$. In other words, $Z \sim
\text{IBP}(\alpha)$. Note that $\alpha \propto K_+$, where $K_+$ is the final
number of sampled dishes (occupied columns). Figure 2.1 shows a draw from an
IBP(10) with 50 rows. The white squares are 1, indicating that a dish was
taken; black squares are 0, indicating that a dish was not taken. \\
\beginmyfig
  \caption{IBP($N=50$, $\alpha=10$)}
  \includegraphics{images/ibp.pdf}
\endmyfig

\noindent
The probability of any particular matrix produced from this process is
\begin{equation}
  P(\bm{Z}) = \frac{\alpha^{K_+}}{\prodl{i}{1}{N} {K_1}^{(i)}!} 
              exp\{-\alpha H_N\}\prodl{k}{1}{K_+}
              \frac{(N-m_k)!(m_k-1)!}{N!},
\end{equation}
where $H_N$ is the harmonic number, $\suml{i}{1}{N}\ds\frac{1}{i}$, $K_+$ is
the number of non-zero columns in $\bm Z$, $m_k$ is the $k^{th}$ column sum of
$\bm Z$, and $K_1^{(i)}$ is the ``number of new dishes" sampled by customer $i$.\\

\subsection{Gibbs Sampler for Indian Buffet Process}
One way to get a draw from the IBP($\alpha$) is to simulate the process according to 
the description above. Another way is to implement a Gibbs sampler as follows:

\begin{enumerate}
  \item Start with an arbitrary binary matrix of $N$ rows
  \item For each row, $i$,
  \begin{enumerate}
    \item For each column, $k$,
    \item if $m_{-i,k}$ = $0$, delete column $k$. Otherwise,
    \item set $z_{ik}$ to $0$
    \item set $z_{ik}$ to $1$ with probability $P(z_{ik}=1|\bm{z_{-i,k}}) = \frac{m_{-i,k}}{i}$
    \item at the end of row $i$, add Poisson($\ds\frac{\alpha}{N}$) columns of $1$'s
  \end{enumerate}
  \item iterate step 2 a large number of times
\end{enumerate}
We can likewise incorporate this Gibbs sampler to sample from the posterior distribution P($\bm{Z|X}$)
where $\bm Z \sim \text{IBP}(\alpha)$ by sampling from the complete conditional
\begin{equation}
  P(z_{ik}=1|\bm{Z_{-(ik)},X})  \propto p(\bm{X|Z}) P(z_{ik}=1|\bm{Z_{-(ik)}}).
\end{equation}\\


\noindent
Note that a conjugate prior for $\alpha$ is a Gamma distribution.
\[
  \begin{array}{rcl}
    \bm Z|\alpha & \sim & \text{IBP}(\alpha)\\
          \alpha & \sim & \text{Gamma}(a,b), \text{where $b$ is the scale parameter}\\
    & & \\
    p(\alpha|\bm Z) & \propto & p(\bm Z|\alpha) p(\alpha)\\
    p(\alpha|\bm Z) & \propto & \alpha^{K_+} e^{-\alpha H_N}  
                                \alpha^{a-1} e^{-\alpha/b}\\
    p(\alpha|\bm Z) & \propto & \alpha^{a+K_+-1} e^{-\alpha(1/b+H_N)}\\
  \end{array}
\]
\begin{equation}
  \alpha|\bm Z  \sim  \text{Gamma}(a+K_+, (1/b+H_N)^{-1})
\end{equation}

% IBP Implementation.
\subsection{Example: Linear-Gaussian Latent Feature Model with Binary Features}
Suppose, we observe an $N \times D$ matrix $\bm X$, and we believe
\[
  \bm X = \bm{ZA} + \bm E,
\]
where $\bm Z|\alpha \sim IBP(\alpha)$, 
$\bm A \sim MVN(\bm 0,{\sigma_A}^2\textbf{I})$, 
$\bm E \sim MVN(\bm 0,{\sigma_X}^2\textbf{I})$, and
$\alpha \sim \text{Gamma}(a,b)$,
%$\sigma_A=1$, and $\sigma_X=.5$.\\

\noindent
It can be shown that
\begin{equation}  
  p(\bm{X|Z}) = \ds\frac{1}{(2\pi)^{ND/2}\sigma_X^{(N-K)D}\sigma_A^{KD}
                            |\bm Z^T \bm Z+({\frac{\sigma_X}{\sigma_A}})^2
                            \textbf{I}|^{D/2}}
                \exp\{-\frac{1}{2\sigma_X^2}tr(\bm X^T
                (\textbf{I}-\bm Z(\bm Z^T\bm Z+({\frac{\sigma_X}{\sigma_A}})^2
                            \textbf{I})^{-1}\bm Z))
                \bm X\}            
\end{equation}
Now, we can use equation (2) to implement a Gibbs sampler to draw from the 
posterior $\bm{Z|X},\alpha$.

\subsection{Data \& Results}
Each of ten students created a $6 \times 6$ binary image. To the
image, Gaussian noise (mean=0,variance=.25) was added to each cell of the
binary image. Gaussian noise was added to the same image to generate 10 $6
\times 6$ images. These images were turned into $1 \times 36$ row vectors. All
these vectorized images were stacked together to form one large $100 \times 36$
matrix.  (Note that I did not know the design of these images before hand. The
images could be letters, numbers, interesting patterns, etc.)
Figure2 displays the data from the ten students.\\

\beginmyfig
  \vspace{-5mm}
  \caption{Data From Ten Students}
  \vspace{-2mm}
  \includegraphics{images/Y.pdf}
  \vspace{-15mm}
\endmyfig

\noindent
A Gibbs sampler was implemented to retrieve posterior distributions for $\bm Z$,
$\bm A$, and $\alpha$. $\alpha$ was initially set to $1$, and the posterior for
$\alpha$ was obtained by equation (3) with prior distribution $Gamma(3,2)$.
The parameters were chosen such that $\alpha$ was centered at 6 and had a
variance of 12, as I believed I may have many latent features, but my
uncertainty was high.  Equation(2) was used to retrieve the posterior
distribution for $\bm Z$ (a collection of binary matrices). After 5000
iterations, the trace plot for the number of columns in the binary matrices
drawn were plotted (See Figure 3). Diagnostics for a cell by cell trace plot
could also be plotted, but may be too difficult to analyze as the dimensions of
the matrices are changing, and the matrices are large. The execution time was
about 4 hours.\\
\beginmyfig
  \caption{}
  \includegraphics{images/traceplot.pdf}
  \vspace{-15mm}
\endmyfig
\beginmyfig
  \caption{}
  \includegraphics{images/postAlpha.pdf}
\endmyfig
\noindent
The number of columns in $\bm Z$ appears to have converged to 9. This means
that the number of latent features discovered appears to be 9. This is
reasonable as the image $\bm X$ is comprised of 10 students' images. A burn-in
of the first 1000 draws were removed. Then, the 4000 $\bm Z$ matrices were
superimposed, summed element by element, and divided by 4000. Cells that had
values $> .9$ were set to 1; and 0 otherwise. This is not necessary, but this
removes the columns that were not likely to exist. To elaborate, from the trace
plot (Figure3), we see that after burn-in, there is one instance where the
number of columns in $\bm Z$ was $10$.  One instance out of 4000 is not
significantly large enough to say that \textit{that} latent feature is 
generating the observed data. So, the tenth column was removed. The resulting
matrix, I will call the posterior mean for $\bm Z$.  The trace plot for
$\alpha$ (Figure 4) shows that $\alpha$ appears to have converged, with mean
$=2.08$ and variance $=.359$.  Figure 5 shows the posterior mean for $\bm Z$.
We can interpret the matrix in the following way. All the observations are 
being generated by the first column (feature). The $11^{th}$ through $20^{th}$
observations in $\bm X$ are being generated by the second column, etc.
The posterior mean for $\bm A$ calculated as $E[\bm{A|X,Z}] = (\bm Z^T\bm Z +
\frac{\sigma_X^2}{\sigma_A^2} \textbf{I})^{-1}\bm Z^T \bm X$, and is shown
in Figure 6 and Figure 7. Figure 6 shows the matrix in a $10 \times 36$ form;
Figure 7 shows the matrix in $6 \times 6$ form. That is, each row in $\bm A$
were back-transformed into its matrix form.\\

\beginmyfig
  \caption{}
  \includegraphics{images/postZ.pdf}
  \vspace{-10mm}
\endmyfig

\beginmyfig
  \caption{}
  \includegraphics{images/postA.pdf}
\endmyfig
\beginmyfig
  \caption{The latent features turned back into $6 \times 6$ images.}
  %\includegraphics[height=1\textwidth]{images/postA66.pdf}
  \includegraphics{images/postA66.pdf}
\endmyfig

\noindent
Now, we can predict the latent features generating each observation by
multiplying the posterior mean of $\bm Z$ by the posterior mean of $\bm A$. We
will call this matrix the posterior mean of $\bm {ZA}$. Each row in this matrix
will reveal the latent structures generating the observation in the respective
row of $\bm X$. Figure 8 shows the latent structure learned from the data. 
For each observation, the data and latent structures are layed side by side
for a visual comparison. The features learned were similar to the images
created by the ten students.


\begin{figure}\begin{center}
  \caption{The latent features for each student}
  %\includegraphics[height=1\textwidth]{images/postA66.pdf}
  \includegraphics[height=1\textwidth]{images/postFriends.pdf}
\end{center}\end{figure}


\section{Distance Dependent Indian Buffet Process}
%Currently, there exists a distance dependent Indian buffet process (dd-IBP), 
%constructed by Gershman, Frazier, and Blei (2012). We wish to create a model 
%similar to it. We will first review the ddIBP.





%The applications of the Indian buffet distribution include defining priors

% What is the IBP? # Done
% Gibbs sampler to draw from prior # Done
% Gibbs sampler to draw from posterior # Done
% What is it used for?
% Where are the applications?
% Why would it be useful to include distance information?
% ddIBP?

